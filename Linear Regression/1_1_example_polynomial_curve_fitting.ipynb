{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Example: Polynomial Curve Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named matplotlib.pyplot",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-e5793d37f713>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPolynomialFeatures\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named matplotlib.pyplot"
     ]
    }
   ],
   "source": [
    "import os\r\n",
    "import functools\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "import pandas as pd\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "from sklearn.preprocessing import PolynomialFeatures\r\n",
    "\r\n",
    "# if \"OUTPUT_RETINA_IMAGES\" in os.environ:\r\n",
    "    # %config InlineBackend.figure_format = \"retina\"\r\n",
    "# \r\n",
    "# plt.rcParams[\"axes.spines.right\"] = False\r\n",
    "# plt.rcParams[\"axes.spines.top\"] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate a dataset using $sin(2\\pi x)$ function but also add some observation noise to reflect the true world !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-c424ee93a076>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_noisy_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_plot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_plot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'lime'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"$\\sin(2\\pi x)$\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'o'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'blue'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"train\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxmax\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinewidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinestyle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'--'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'k'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "N = 10\r\n",
    "SEED = 1234\r\n",
    "SCALE = 0.25\r\n",
    "\r\n",
    "np.random.seed(SEED)\r\n",
    "\r\n",
    "def true_fun(x):\r\n",
    "    return np.sin(2 * np.pi * x)\r\n",
    "\r\n",
    "def generate_noisy_data(x, scale=SCALE):\r\n",
    "    y = true_fun(x) + np.random.normal(scale=scale, size=x.shape)\r\n",
    "    return y\r\n",
    "\r\n",
    "x_plot = np.arange(0, 1.01, 0.01)\r\n",
    "y_plot = true_fun(x_plot)\r\n",
    "\r\n",
    "# points with noise, will act as train data\r\n",
    "x_train = np.linspace(0, 1, N)\r\n",
    "y_train = generate_noisy_data(x_train)\r\n",
    "\r\n",
    "plt.plot(x_plot, y_plot, color='lime', label=\"$\\sin(2\\pi x)$\")\r\n",
    "plt.scatter(x_train, y_train, marker='o', color='blue', label=\"train\")\r\n",
    "plt.hlines(y=y_train[3], xmin=-2, xmax=x_train[3], linewidth=1, linestyle='--', color = 'k')\r\n",
    "plt.vlines(x=x_train[3], ymin=-2, ymax=y_train[3], linewidth=1, linestyle='--', color = 'k')\r\n",
    "\r\n",
    "plt.text(-0.1, y_train[3], \"$y_{train}$\", fontsize=15)\r\n",
    "plt.text(x_train[3] - 0.01, -1.7, \"$x_{train}$\", fontsize=15)\r\n",
    "plt.yticks( [-1.5, 0.0, 1.5] )\r\n",
    "plt.xticks( [0.0, 0.5, 1.0] )\r\n",
    "plt.xlim(-0.05, 1.05)\r\n",
    "plt.ylim(-1.5, 1.5)\r\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use different models of regression to determine the weights. These models will differ by the degree of polynomial so as a first step let's transform our features ($x_{train}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_features(X, m):\n",
    "    \"\"\" Create a polynomial of specified degrees \"\"\"\n",
    "    return PolynomialFeatures(degree=m).fit_transform(X.reshape(-1, 1))\n",
    "\n",
    "# examples of creating the polynomial features\n",
    "features_m_0 = transform_features(x_train, m=0)\n",
    "features_m_1 = transform_features(x_train, m=1)\n",
    "features_m_3 = transform_features(x_train, m=3)\n",
    "features_m_9 = transform_features(x_train, m=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_regression_fit(features, y_train):\n",
    "    A_t = features.T @ features \n",
    "    weight_vector = np.linalg.solve(A_t, features.T @ y_train)\n",
    "    return weight_vector\n",
    "\n",
    "def multiple_regression_predict(features, weight_vector):\n",
    "    return np.dot(features, weight_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we will invoke above fit & predict methods in a loop for all\r\n",
    "# the polynomials\r\n",
    "\r\n",
    "# test data\r\n",
    "# x that we used to draw out sin function will act as the test data\r\n",
    "\r\n",
    "def regress(m, x, y):\r\n",
    "    poly_train_features = transform_features(x, m=m)\r\n",
    "    return multiple_regression_fit(poly_train_features, y)\r\n",
    "    \r\n",
    "def predict(m, x, weight_vector):\r\n",
    "    poly_test_features = transform_features(x, m=m)\r\n",
    "    return multiple_regression_predict(poly_test_features, weight_vector)\r\n",
    "\r\n",
    "def regress_and_predict(m, x_train, y_train, x_test):\r\n",
    "    weight_vector = regress(m, x_train, y_train)\r\n",
    "    return predict(m, x_test, weight_vector)\r\n",
    "\r\n",
    "fig, ax = plt.subplots(2, 2, sharex=True, sharey=True)\r\n",
    "\r\n",
    "plt.xlim(-0.05, 1.05)\r\n",
    "plt.ylim(-1.5, 1.5)\r\n",
    "\r\n",
    "def plot(m, x, y_predict, ax):\r\n",
    "    # true function\r\n",
    "    ax.plot(x_plot, y_plot, color='lime', label=\"$\\sin(2\\pi x)$\")\r\n",
    "\r\n",
    "    # training data\r\n",
    "    ax.scatter(x_train, y_train, marker='o', color='blue', label=\"Train\")\r\n",
    "\r\n",
    "    # Show the poly degree\r\n",
    "    ax.text(0.65, 0.8, f\"M={m}\")\r\n",
    "\r\n",
    "    # finally the predictions\r\n",
    "    ax.plot(x, y_predict, color='red', label=\"Test\")\r\n",
    "\r\n",
    "\r\n",
    "x_test = np.linspace(0, 1, 100)\r\n",
    "y_test = generate_noisy_data(x_test)\r\n",
    "\r\n",
    "partial_rp = functools.partial(regress_and_predict, \r\n",
    "                        x_train=x_train, \r\n",
    "                        y_train=y_train, \r\n",
    "                        x_test=x_test)\r\n",
    "\r\n",
    "results = list(map(partial_rp, [0,1,3,9]))\r\n",
    "\r\n",
    "plot(m=0, x=x_test, y_predict=results[0], ax=ax[0][0])\r\n",
    "plot(m=1, x=x_test, y_predict=results[1], ax=ax[0][1])\r\n",
    "plot(m=3, x=x_test, y_predict=results[2], ax=ax[1][0])\r\n",
    "plot(m=9, x=x_test, y_predict=results[3], ax=ax[1][1])\r\n",
    "\r\n",
    "# plt.legend(bbox_to_anchor=(1.05, 1.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes -\n",
    "\n",
    "* Red curve shows the fitted curve.\n",
    "* M=0 fails to create the sin curve\n",
    "* M=1 also failed\n",
    "* M=3 seems to re-create the curve close to original function\n",
    "* M=9 creates a curve that passes through all the training data but fails to approximate sin and is very wiggly. This is overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating residual value of $E(w^*)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rmse(a, b):\n",
    "    return np.sqrt(np.mean(np.square(a - b)))\n",
    "\n",
    "# try polynomial of degrees from 0 to 10\n",
    "M = range(0, 10)\n",
    "\n",
    "weight_vectors = []\n",
    "for m in M:\n",
    "    w = regress(m=m, x=x_train, y=y_train)\n",
    "    weight_vectors.append(w)\n",
    "    \n",
    "def compute_errors(x, y, m, weight_vector):\n",
    "    y_predict = predict(m=m, x=x, weight_vector=weight_vector)\n",
    "    return rmse(y_predict, y)\n",
    "\n",
    "def accumate_errors(x, y):\n",
    "    m_w = zip(M, weight_vectors)\n",
    "    errors = []\n",
    "    for (m, w) in m_w:\n",
    "        errors.append(compute_errors(x, y, m, w))\n",
    "    return errors\n",
    "\n",
    "x_test = np.arange(0, 1.01, 0.01)\n",
    "y_test = generate_noisy_data(x_test)\n",
    "\n",
    "training_errors = accumate_errors(x_train, y_train)\n",
    "test_errors = accumate_errors(x_test, y_test)\n",
    "\n",
    "plt.plot(training_errors, 'o-', mfc=\"none\", mec=\"b\", ms=10, c=\"b\", label=\"Training\")\n",
    "plt.plot(test_errors, 'o-', mfc=\"none\", mec=\"r\", ms=10, c=\"r\", label=\"Test\")\n",
    "plt.legend()\n",
    "plt.yticks( [0,0.5,1] )\n",
    "plt.xticks( [0,3,6,9] )\n",
    "plt.xlabel(\"degree\")\n",
    "plt.ylabel(\"RMSE\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes -\n",
    "\n",
    "* For M = 9, the training set error goes to zero, for test set it shoots up\n",
    "* A polynomial of higher degree contains the functions for lower so why this behavior ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the weight vectors of various polynomials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_vector_m_0 = regress(m=0, x=x_train, y=y_train)\n",
    "weight_vector_m_1 = regress(m=1, x=x_train, y=y_train)\n",
    "weight_vector_m_3 = regress(m=3, x=x_train, y=y_train)\n",
    "weight_vector_m_9 = regress(m=9, x=x_train, y=y_train)\n",
    "\n",
    "# makeing them same dimension so \n",
    "# as display using a pandas frame\n",
    "def pad_weight_vectors(weight_vector):\n",
    "    while len(weight_vector) <= 9:\n",
    "        weight_vector = np.append(weight_vector, 0.)\n",
    "    return weight_vector\n",
    "\n",
    "weight_vector_m_0 = pad_weight_vectors(weight_vector_m_0)\n",
    "weight_vector_m_1 = pad_weight_vectors(weight_vector_m_1)\n",
    "weight_vector_m_3 = pad_weight_vectors(weight_vector_m_3)\n",
    "weight_vector_m_9 = pad_weight_vectors(weight_vector_m_9)\n",
    "\n",
    "df = pd.DataFrame([weight_vector_m_0,weight_vector_m_1,weight_vector_m_3,weight_vector_m_9]).transpose()\n",
    "df.columns = ['M=0', 'M=1', 'M=3', 'M=9']\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes -\n",
    "\n",
    "* As M increases, the magnitude of weights is becoming larger\n",
    "* In particular for M=9, the weights become very large +ive and -ive so that corresponding polynomial function matches the data point exactly\n",
    "\n",
    "> Intuitively, what is happening is that the more flexible polynomials with larger values of M are becoming increasingly tuned to the random noise on the target values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Increasing the datasize reduces overfitting\n",
    "\n",
    "Here we use M=9 that resulted in overfitting but increase the training dataset size. We will try N=15 and N=100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_overfit(N, m=9):\n",
    "    x_train = np.linspace(0, 1, N)\n",
    "    y_train = generate_noisy_data(x_train)\n",
    "\n",
    "    poly_train_features = transform_features(x_train, m=m)\n",
    "    weight_vector = multiple_regression_fit(poly_train_features, y_train)\n",
    "\n",
    "    return x_train, y_train, weight_vector\n",
    "   \n",
    "def regress_and_plot(x):\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, sharey=True, figsize=(10, 4))\n",
    "\n",
    "    plt.xlim(-0.05, 1.05)\n",
    "    plt.ylim(-2.0, 2.0)\n",
    "\n",
    "    for i, n in enumerate([15, 100]):\n",
    "        # get weight vector\n",
    "        x_train, y_train, weight_vector = analyze_overfit(N=n)\n",
    "        # make predictions\n",
    "        poly_test_features = transform_features(x, m=9)\n",
    "        y_predict = multiple_regression_predict(poly_test_features, weight_vector)\n",
    "\n",
    "        # true function\n",
    "        ax[i].plot(x_plot, y_plot, color='lime', label=\"$\\sin(2\\pi x)$\")\n",
    "\n",
    "        # training data\n",
    "        ax[i].scatter(x_train, y_train, marker='o', color='blue', label=\"Train\")\n",
    "\n",
    "        # Show the poly degree\n",
    "        ax[i].text(0.65, 0.8, f\"N={n}\")\n",
    "\n",
    "        # finally the predictions\n",
    "        ax[i].plot(x, y_predict, color='red', label=\"Test\")\n",
    "\n",
    "regress_and_plot(x=x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plots, it is evident that as we increased the data size to 100, we got rid of overfitting.\n",
    "\n",
    "> One rough heuristic that is sometimes advocated is that the number of data points should be no less than some multiple (say 5 or 10) of the number of adaptive parameters in the model.\n",
    "\n",
    "It is indeed a nuisance to have this delicate dance between number of parameters and dataset size. *Could Bayesian Inference help ?* Authors says - **Yes**\n",
    "\n",
    "At the moment we restrict ourselves to current situation only. Here because we had a synthetic dataset we could increase the number of training samples at will but real world is differnt. So next question to ask is - *Could we reduce overfitting when we have less number of training examples ?*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "We observed that the weights started to reach larger values for M=9; so one technique to circumvent is to add penalty term to the error function in order to discouragethe coefficients from reaching large values.\n",
    "\n",
    "The new error function then takes the form \n",
    "\n",
    "$$\n",
    "E(\\mathbf{w}) = \\frac{1}{2} \\sum_{n=1}^N \\{y(x_n, \\mathbf{w}) - t_n\\}^2 + \\frac{\\lambda}{2} ||\\mathbf{w}||^2\n",
    "$$\n",
    "\n",
    "The $\\lambda$ in the above equation controls the contribution of the regularization to the overall error. This is a hyper-parameter.\n",
    "\n",
    "Above can be done in closed form.\n",
    "\n",
    "This method is known by various other names as well.\n",
    "\n",
    "In statistics, it is known as *shrinkage* since we are reducing the values of $\\mathbf{w}$.\n",
    "\n",
    "In machine learning, we call it *weight decay*.\n",
    "\n",
    "In this particular case of quadratics weight decay, it is also called *Ridge Regression*\n",
    "\n",
    "After some mathematical rigor, the closed form solution is -\n",
    "\n",
    "$$ \\beta^{ridge} = (X^T X + \\lambda I)^{-1} X^T Y $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we will modify our multiple regression fit function\n",
    "# to take this into account. Rest of the functions defined above\n",
    "# should still be usable\n",
    "\n",
    "def multiple_regression_ridge_fit(features, y_train, lamb):\n",
    "    # Note - addition of second term for A_t\n",
    "    A_t = features.T @ features + lamb * np.eye(features.shape[1], features.shape[1])\n",
    "    weight_vector = np.linalg.solve(A_t, features.T @ y_train)\n",
    "    return weight_vector\n",
    "\n",
    "def regress_ridge(m, x, y, lamb):\n",
    "    poly_train_features = transform_features(x, m=m)\n",
    "    # get weights\n",
    "    weight_vector = multiple_regression_ridge_fit(poly_train_features, y, lamb)\n",
    "    return weight_vector\n",
    "\n",
    "def regress_ridge_and_plot(x):\n",
    "\n",
    "    fig, ax = plt.subplots(1, 3, sharey=True, figsize=(10,4))\n",
    "\n",
    "    plt.xlim(-0.05, 1.05)\n",
    "    plt.ylim(-2.0, 2.0)\n",
    "\n",
    "    for i, lamb in enumerate([-1000, -18, 0]):\n",
    "\n",
    "        weight_vector_m_9_ridge = regress_ridge(m=9, x=x_train, y=y_train, lamb=np.exp(lamb))\n",
    "        y_predict = predict(m=9, x=x, weight_vector=weight_vector_m_9_ridge)\n",
    "\n",
    "        # true function\n",
    "        ax[i].plot(x_plot, y_plot, color='lime', label=\"$\\sin(2\\pi x)$\")\n",
    "\n",
    "        # training data\n",
    "        ax[i].scatter(x_train, y_train, marker='o', color='blue', label=\"Train\")\n",
    "\n",
    "        # Show the poly degree\n",
    "        ax[i].text(0.65, 0.8, f\"$ln \\ \\lambda$={lamb}\")\n",
    "\n",
    "        # finally the predictions\n",
    "        ax[i].plot(x, y_predict, color='red', label=\"Test\")\n",
    "\n",
    "regress_ridge_and_plot(x=x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ln \\ \\lambda$ = -1000 corresponds to having no regualrizer. In the above plot, are shown the effect of 3 different regularizers. Clearly out of these 3, $ln \\ \\lambda$ = -18 seems to do the trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_vector_m_9_0 = regress_ridge(m=9, x=x_train, y=y_train, lamb=np.exp(-1000))\n",
    "weight_vector_m_9_1 = regress_ridge(m=9, x=x_train, y=y_train, lamb=np.exp(-18))\n",
    "weight_vector_m_9_2 = regress_ridge(m=9, x=x_train, y=y_train, lamb=np.exp(0))\n",
    "\n",
    "weight_vector_m_9_0 = pad_weight_vectors(weight_vector_m_9_0)\n",
    "weight_vector_m_9_1 = pad_weight_vectors(weight_vector_m_9_1)\n",
    "weight_vector_m_9_2 = pad_weight_vectors(weight_vector_m_9_2)\n",
    "\n",
    "df = pd.DataFrame([weight_vector_m_9_0,weight_vector_m_9_1,weight_vector_m_9_2]).transpose()\n",
    "df.columns = ['ln lambda=-1000', 'ln lambda=-18', 'ln lambda=0', ]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that, as the value of $\\lambda$ increases, the typical magnitude of the coefficients gets smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamb = range(-40, -10, 1)\r\n",
    "\r\n",
    "train_errors = []\r\n",
    "test_errors = []\r\n",
    "for l in lamb:\r\n",
    "    weight_vector_m_9_ridge = regress_ridge(m=9, x=x_train, y=y_train, lamb=np.exp(l))\r\n",
    "\r\n",
    "    y_train_predict = predict(m=9, x=x_train, weight_vector=weight_vector_m_9_ridge) \r\n",
    "    y_test_predict = predict(m=9, x=x_test, weight_vector=weight_vector_m_9_ridge)\r\n",
    "\r\n",
    "    train_errors.append(rmse(y_train, y_train_predict))\r\n",
    "    test_errors.append(rmse(y_test, y_test_predict))\r\n",
    "\r\n",
    "plt.plot(lamb, train_errors, 'o-', mfc=\"none\", mec=\"b\", ms=10, c=\"b\", label=\"Training\")\r\n",
    "plt.plot(lamb, test_errors, 'o-', mfc=\"none\", mec=\"r\", ms=10, c=\"r\", label=\"Test\")\r\n",
    "plt.legend()\r\n",
    "plt.yticks( [0,0.5,1] )\r\n",
    "plt.xlim((-40, -10))\r\n",
    "plt.xlabel(\"$ln \\lambda$\")\r\n",
    "plt.ylabel(\"RMSE\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2.7.15 64-bit",
   "name": "python2715jvsc74a57bd05c3b02fa24688d32a1d09351b7f8eac82a87272ad081601c73caead00a8e1fc4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "metadata": {
   "interpreter": {
    "hash": "5c3b02fa24688d32a1d09351b7f8eac82a87272ad081601c73caead00a8e1fc4"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}